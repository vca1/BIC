""" BUILD A SIMPLE ANN
code based on joanby  (GitHub)
python"""

# DATA PREPROCESSING

# Import libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

!pip install sklearn
!pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git
!pip install keras
!pip install tensorflow

# Import dataset > Go to  "Upload file from Drive" 
dataset = pd.read_csv('')
X = dataset.iloc[:, :] #choose the relevant columns for our prediction
y = dataset.iloc[:, :] #choose the column we want to predict

""" Convert categorical data. Creating labels and dummy variables (OneHotEncoder)
 When more than one categorical data> for the OneHotEncoder, we remove one of the dummy categories.
 In the code below, two categorical data are considered. # and ## must be replaced by the column number""" 

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
labelencoder_X_# = LabelEncoder()
X[:,#] = labelencoder_X_#.fit_transform(X[:, :])
labelencoder_X_## = LabelEncoder()
X[:, ##] = labelencoder_X_##.fit_transform(X[:, :])
onehotencoder = ColumnTransformer(
    [('one_hot_encoder', OneHotEncoder(categories='auto'), [])],
      remainder = 'passthrough')
X = onehotencoder.fit_transform(X)
X = X[:, #:]

# Divide the data intro Training and Test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)

# We need to scalate the data (normalization)
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# BUILD THE ANN 
# Import other libraries
import keras
from keras.models import Sequential
from keras.layers import Dense # for adding hidden layers
from keras.layers imoprt Dropout # for adjusting the previous hidden layer

# Import functions from libraries for building the classifier
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score

# Build the classifier
def build_classifier():
  classifier =Sequential()
  classifier.add(Dense(units= 6, kernel_initializer= "uniform", activation= "relu", input_dim = 11))
  classifier.add(Dropout(p = 0.1))
  classifier.add(Dense(units= 6, kernel_initializer= "uniform", activation= "relu"))
  classifier.add(Dropout(p =0.1))
  classifier.add(Dense(units= 6, kernel_initializer= "uniform", activation= "sigmoid"))
  classifier.compile(optimizer = "adam", loss = "binary_crossentropy", metrics = ["accuracy"])
  return classifier

# Specify for how many epochs we want to train our dataset
classifier = KerasClassifier(build_fn =build_classifier, batch_size = 10, nb_epoch = 100)
accuracies = cross_val_score(estimator=classifier, X = X_train, y = y_train, cv =10, n_jobs=-1, verbose =1)

# Check accuracy of the training model 
mean = accuracies.mean()
variance = accuracies.std()
print(mean)
print(variance)

# ADJUST THE ANN

# Check which classifier works better for our dataset 
from sklearn.model_selection import GridSearchCV

# minor changes to our build_classifier function above 
def build_classifier(optimizer):
  classifier =Sequential()
  classifier.add(Dense(units= 6, kernel_initializer= "uniform", activation= "relu", input_dim = 11))
 
  classifier.add(Dense(units= 6, kernel_initializer= "uniform", activation= "relu"))
 
  classifier.add(Dense(units= 6, kernel_initializer= "uniform", activation= "sigmoid"))
  classifier.compile(optimizer = optimizer, loss = "binary_crossentropy", metrics = ["accuracy"])
  return classifier
  
  #
  classifier = KerasClassifier(build_fn = build_classifier)
  
  # Adjust the hyperparameters to find out which combination works better for us
  parameters = {
    'batch_size' : [25, 32],
    'nb_epoch' : [100, 500],
    'optimizer' : ['adam', 'rmsprop']
}

# Define the search for best hyperparameters. This can take hours
grid_search = GridSearchCV(estimator = classifier,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv =10)
grid_search = grid_search.fit(X_train, y_train)

best_parameters = grid_search.best_params_
best_accuracy = grid_search.best_score_
  

